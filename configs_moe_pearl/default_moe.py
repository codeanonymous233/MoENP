# default PEARL experiment settings
# all experiments should modify these settings only as needed
default_moe_config = dict(
    env_name='cheetah-dir',
    n_train_tasks=2,
    n_eval_tasks=2,
    latent_size=5, 
    num_experts=3, 
    experts_in_gates=False, 
    dim_logit_h=100, 
    num_logit_layers=2, 
    temperature=0.05, 
    hard=True, 
    gumbel_max=False,
    net_size=300, 
    path_to_weights=None, 
    env_params=dict(
        n_tasks=2, 
        randomize_tasks=True, 
    ),
    algo_params=dict(
        meta_batch=16, 
        num_iterations=500, 
        num_initial_steps=2000, 
        num_tasks_sample=5, 
        num_steps_prior=400, 
        num_steps_posterior=0, 
        num_extra_rl_steps_posterior=400, 
        num_train_steps_per_itr=2000, 
        num_evals=2, 
        num_steps_per_eval=600, 
        batch_size=256, 
        embedding_batch_size=64, 
        embedding_mini_batch_size=64, 
        max_path_length=200, 
        discount=0.99,
        soft_target_tau=0.005,
        policy_lr=3E-4,
        qf_lr=3E-4,
        vf_lr=3E-4,
        context_lr=3e-4,
        kl_lambda0=1.0,
        kl_lambda1=1.0,  
        reward_scale=5., 
        sparse_rewards=False, 
        whether_moe_critic_loss=False, 
        use_information_bottleneck=True, 
        use_next_obs_in_context=False, 
        update_post_train=1, 
        num_exp_traj_eval=1,
        recurrent=False, 
        dump_eval_paths=False, 
    ),
    util_params=dict(
        base_log_dir='output',
        use_gpu=True,
        gpu_id=0,
        debug=False,
        docker=False, 
    )
)


